"""
Test submission for hierarchical entity extraction on the dataset.
Processes en_data_test1.json and outputs results in submit_example.json format.
"""
import asyncio
import json
import os
import sys
import logging
import uuid
from typing import Dict, List, Any
from collections import defaultdict
from datetime import datetime
from logging.handlers import RotatingFileHandler
from dotenv import load_dotenv

# Âä†ËΩΩ .env ÈÖçÁΩÆÊñá‰ª∂
load_dotenv()

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Ëé∑ÂèñÈ°πÁõÆÊ†πÁõÆÂΩï
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# ‰ªéÁéØÂ¢ÉÂèòÈáèËØªÂèñÈÖçÁΩÆ
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api-inference.modelscope.cn/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "Qwen/Qwen3-235B-A22B-Instruct-2507")
EXTRACT_LANGUAGE = os.getenv("EXTRACT_LANGUAGE", "Chinese")

# ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàÂÖºÂÆπÊóß‰ª£Á†ÅÔºâ
os.environ["OPENAI_API_BASE"] = OPENAI_API_BASE
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_MODEL"] = OPENAI_MODEL
os.environ["EXTRACT_LANGUAGE"] = EXTRACT_LANGUAGE

# Êñá‰ª∂Ë∑ØÂæÑÈÖçÁΩÆÔºàÊîØÊåÅÁõ∏ÂØπË∑ØÂæÑÔºâ
INPUT_FILE = os.path.join(PROJECT_ROOT, os.getenv("INPUT_FILE", "data/zh_data_dev1.json"))
OUTPUT_FILE = os.path.join(PROJECT_ROOT, os.getenv("OUTPUT_FILE", "output/submit_results.json"))
TYPE_DICT_PATH = os.path.join(PROJECT_ROOT, os.getenv("TYPE_DICT_PATH", "data/coarse_fine_type_dict.json"))

# Êó•ÂøóÈÖçÁΩÆ
LOG_DIR = os.path.join(PROJECT_ROOT, os.getenv("LOG_DIR", "logs"))
LOG_FILE_MAX_BYTES = int(os.getenv("LOG_FILE_MAX_BYTES", str(10 * 1024 * 1024)))
LOG_FILE_BACKUP_COUNT = int(os.getenv("LOG_FILE_BACKUP_COUNT", "5"))


def setup_logger(name: str = "entity_extractor", log_level: int = logging.INFO) -> logging.Logger:
    """
    ÈÖçÁΩÆÊó•ÂøóÁ≥ªÁªüÔºàÁ∫øÁ®ãÂÆâÂÖ®Ôºâ
    
    Args:
        name: LoggerÂêçÁß∞
        log_level: Êó•ÂøóÁ∫ßÂà´
        
    Returns:
        ÈÖçÁΩÆÂ•ΩÁöÑloggerÂØπË±°
    """
    # ÂàõÂª∫logsÁõÆÂΩï
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # ÂàõÂª∫logger
    logger = logging.getLogger(name)
    logger.setLevel(log_level)
    
    # ÈÅøÂÖçÈáçÂ§çÊ∑ªÂä†handler
    if logger.handlers:
        return logger
    
    # Êó•ÂøóÊ†ºÂºè
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ÊéßÂà∂Âè∞handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Êñá‰ª∂handler - ËØ¶ÁªÜÊó•Âøó
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    detail_log_file = os.path.join(LOG_DIR, f'extraction_{timestamp}.log')
    file_handler = RotatingFileHandler(
        detail_log_file,
        maxBytes=LOG_FILE_MAX_BYTES,
        backupCount=LOG_FILE_BACKUP_COUNT,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # ÈîôËØØÊó•ÂøóÊñá‰ª∂handler
    error_log_file = os.path.join(LOG_DIR, f'extraction_errors_{timestamp}.log')
    error_handler = RotatingFileHandler(
        error_log_file,
        maxBytes=LOG_FILE_MAX_BYTES,
        backupCount=LOG_FILE_BACKUP_COUNT,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)
    logger.addHandler(error_handler)
    
    logger.info("Êó•ÂøóÁ≥ªÁªüÂàùÂßãÂåñÂÆåÊàê")
    logger.info(f"ËØ¶ÁªÜÊó•Âøó: {detail_log_file}")
    logger.info(f"ÈîôËØØÊó•Âøó: {error_log_file}")
    
    return logger


# ÂàõÂª∫ÂÖ®Â±Älogger
logger = setup_logger()

from common.openai import OpenAIRE
from core.pipeline import extract_from_text
from core.type_manager import get_type_manager, initialize_type_manager
from core.hierarchical_prompts import HIERARCHICAL_PROMPTS


class HierarchicalEntityExtractor:
    """Extractor for hierarchical entity types with coarse-fine mapping."""
    
    def __init__(self, model_name: str = os.environ["OPENAI_MODEL"]):
        """Initialize the extractor with LLM and type manager."""
        self.llm = OpenAIRE(
            model=model_name,
            api_key=os.environ["OPENAI_API_KEY"],
            base_url=os.environ["OPENAI_API_BASE"]
        )
        # Initialize type manager with explicit path to coarse_fine_type_dict.json
        self.type_manager = initialize_type_manager(TYPE_DICT_PATH)
        
        # Verify that the mapping file was loaded correctly
        self._verify_type_mapping_loaded()
        self.language = os.environ["EXTRACT_LANGUAGE"] if "EXTRACT_LANGUAGE" in os.environ else EXTRACT_LANGUAGE
        
    def _verify_type_mapping_loaded(self):
        """Verify that the coarse_fine_type_dict.json file was loaded correctly."""
        if not self.type_manager.coarse_fine_mapping:
            raise FileNotFoundError("Failed to load coarse_fine_type_dict.json file")
        
        logger.info("‚úÖ ÊàêÂäüÂä†ËΩΩÁ±ªÂûãÊò†Â∞ÑÊñá‰ª∂:")
        logger.info(f"   - Á≤óÁ≤íÂ∫¶Á±ªÂûãÊï∞Èáè: {len(self.type_manager.coarse_fine_mapping)}")
        logger.info(f"   - ÊÄªÁ±ªÂûãÊï∞Èáè: {len(self.type_manager.all_types)}")
        
        # Show some examples
        sample_coarse_types = list(self.type_manager.coarse_fine_mapping.keys())[:5]
        logger.info(f"   - Á§∫‰æãÁ≤óÁ≤íÂ∫¶Á±ªÂûã: {sample_coarse_types}")
        
        for coarse_type in sample_coarse_types:
            fine_types = self.type_manager.coarse_fine_mapping[coarse_type]
            logger.info(f"     {coarse_type}: {len(fine_types)} ‰∏™ÁªÜÁ≤íÂ∫¶Á±ªÂûã")
    
    async def llm_func(self, prompt, system_prompt=None, history_messages=None, **kwargs):
        """Wrapper for LLM function."""
        return await self.llm.complete(prompt, system_prompt, history_messages, **kwargs)
    
    def get_types_for_coarse_types(self, coarse_types: List[str]) -> str:
        """
        Get fine-grained types for the given coarse types.
        Only include coarse types that have mappings in the dictionary.
        
        Args:
            coarse_types: List of coarse types from the dataset
            
        Returns:
            Formatted string of fine types for prompt
        """
        fine_types = []
        valid_coarse_types = []
        
        for coarse_type in coarse_types:
            if coarse_type in self.type_manager.coarse_fine_mapping:
                # Get all fine types for this coarse type
                fine_types.extend(self.type_manager.coarse_fine_mapping[coarse_type])
                valid_coarse_types.append(coarse_type)
            else:
                # Skip coarse types that don't have mappings
                logger.warning(f"Ë∑≥ËøáÊú™ÊâæÂà∞Êò†Â∞ÑÁöÑÁ≤óÁ≤íÂ∫¶Á±ªÂûã: {coarse_type}")
        
        # Remove duplicates (no limit on types)
        unique_types = list(set(fine_types))
        
        logger.info(f"‚úÖ ‰ΩøÁî®ÊúâÊïàÁ≤óÁ≤íÂ∫¶Á±ªÂûã: {valid_coarse_types}")
        logger.info(f"üìã ÂØπÂ∫îÁöÑÁªÜÁ≤íÂ∫¶Á±ªÂûãÊï∞Èáè: {len(unique_types)}")
        
        return ", ".join(sorted(unique_types))
    
    def get_coarse_fine_mapping_for_types(self, coarse_types: List[str]) -> Dict[str, List[str]]:
        """
        Get the coarse-fine mapping for the given coarse types.
        Only include coarse types that have mappings with non-empty fine types in the dictionary.
        
        Args:
            coarse_types: List of coarse types from the dataset
            
        Returns:
            Dictionary mapping coarse types to their fine types (only valid ones with non-empty lists)
        """
        mapping = {}
        skipped_types = []
        
        for coarse_type in coarse_types:
            if coarse_type in self.type_manager.coarse_fine_mapping:
                fine_types = self.type_manager.coarse_fine_mapping[coarse_type]
                # Only include if fine_types is not empty
                if fine_types and len(fine_types) > 0:
                    mapping[coarse_type] = fine_types
                else:
                    skipped_types.append(f"{coarse_type} (Êó†ÁªÜÁ≤íÂ∫¶Á±ªÂûã)")
            else:
                skipped_types.append(f"{coarse_type} (Êú™ÊâæÂà∞Êò†Â∞Ñ)")
        
        # Log skipped types if any
        if skipped_types:
            logger.warning(f"  Ë∑≥ËøáÁöÑÁ±ªÂûã: {', '.join(skipped_types)}")
        
        return mapping
    
    async def extract_entities_for_sentence(self, sentence: str, coarse_types: List[str]) -> List[Dict[str, str]]:
        """
        Three-stage extraction with forward and reverse approaches:
        1. Forward: Extract coarse types first, then fine types
        2. Reverse: Extract fine types first, then map to coarse types
        3. Merge: Compare and merge results from both approaches
        
        Args:
            sentence: Input sentence
            coarse_types: Available coarse types for this sentence
            
        Returns:
            List of entities with both coarse and fine types (merged from both approaches)
        """
        # Get coarse-fine mapping for this sentence (only includes types with fine types)
        coarse_fine_mapping = self.get_coarse_fine_mapping_for_types(coarse_types)
        
        if not coarse_fine_mapping:
            logger.warning(f"  Ê≤°ÊúâÂèØÁî®ÁöÑÁ±ªÂûãÊò†Â∞ÑÔºàÊâÄÊúâÁ±ªÂûãÈÉΩÊ≤°ÊúâÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºâÔºåË∑≥ËøáÊäΩÂèñ")
            return []
        
        # Get valid coarse types (those that have fine types)
        valid_coarse_types = list(coarse_fine_mapping.keys())
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üìñ Â§ÑÁêÜÂè•Â≠ê: {sentence[:100]}...")
        logger.info(f"üéØ ÂéüÂßãÁ≤óÁ≤íÂ∫¶Á±ªÂûã: {coarse_types}")
        if len(valid_coarse_types) < len(coarse_types):
            logger.warning(f"  ËøáÊª§ÂêéÂâ©‰Ωô {len(valid_coarse_types)}/{len(coarse_types)} ‰∏™ÊúâÊïàÁ±ªÂûã")
        logger.info(f"‚úÖ ÊúâÊïàÁ≤óÁ≤íÂ∫¶Á±ªÂûãÔºàÊúâÁªÜÁ≤íÂ∫¶Êò†Â∞ÑÔºâ: {valid_coarse_types}")
        
        try:
            # ============ Stage 1: Forward Extraction (Coarse -> Fine) ============
            logger.info(f"\n„ÄêÈò∂ÊÆµ1-Ê≠£Âêë„ÄëÂÖàÊäΩÂèñÁ≤óÁ≤íÂ∫¶ÔºåÂÜçÊäΩÂèñÁªÜÁ≤íÂ∫¶...")
            forward_entities = await self._forward_extraction(sentence, valid_coarse_types, coarse_fine_mapping)
            
            # ============ Stage 2: Reverse Extraction (Fine -> Coarse) ============
            logger.info(f"\n„ÄêÈò∂ÊÆµ2-ÂèçÂêë„ÄëÂÖàÊäΩÂèñÁªÜÁ≤íÂ∫¶ÔºåÂÜçÊò†Â∞ÑÁ≤óÁ≤íÂ∫¶...")
            reverse_entities = await self._reverse_extraction(sentence, coarse_fine_mapping)
            
            # ============ Stage 3: Merge Results ============
            logger.info(f"\n„ÄêÈò∂ÊÆµ3-ÂêàÂπ∂„ÄëÂØπÊØîÂπ∂ÂêàÂπ∂‰∏§ÁßçÊñπÊ≥ïÁöÑÁªìÊûú...")
            merged_entities = self._merge_extraction_results(forward_entities, reverse_entities, sentence)
            
            logger.info(f"\n‚úÖ ÊúÄÁªàÂêàÂπ∂ÂêéÊèêÂèñÂà∞ {len(merged_entities)} ‰∏™ÂÆåÊï¥ÂÆû‰Ωì")
            for entity in merged_entities:
                source = entity.get('source', 'unknown')
                logger.info(f"   - {entity['name']}: {entity['coarse_type']} -> {entity['fine_type']} (Êù•Ê∫ê: {source})")
            
            return merged_entities
            
        except Exception as e:
            logger.error(f" ÊèêÂèñÂ§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def _forward_extraction(self, sentence: str, valid_coarse_types: List[str], coarse_fine_mapping: Dict[str, List[str]]) -> List[Dict[str, str]]:
        """
        Forward extraction: Coarse -> Fine
        
        Args:
            sentence: Input sentence
            valid_coarse_types: List of valid coarse types
            coarse_fine_mapping: Mapping from coarse to fine types
            
        Returns:
            List of entities with coarse and fine types
        """
        logger.info(f"  ‚û°Ô∏è  Ê≠£ÂêëÊäΩÂèñÔºöÁ≤óÁ≤íÂ∫¶ ‚Üí ÁªÜÁ≤íÂ∫¶")
        
        # Extract coarse entities
        coarse_entities = await self._extract_coarse_entities(sentence, valid_coarse_types)
        
        if not coarse_entities:
            logger.warning(f"    Êú™ÊèêÂèñÂà∞ÂÆû‰ΩìÔºàÊ≠£ÂêëÔºâ")
            return []
        
        logger.info(f"  ‚úÖ Ê≠£ÂêëÊèêÂèñÂà∞ {len(coarse_entities)} ‰∏™Á≤óÁ≤íÂ∫¶ÂÆû‰Ωì")
        for entity in coarse_entities:
            logger.info(f"     - {entity['name']} -> {entity['coarse_type']}")
        
        # Extract fine types for each entity
        result_entities = await self._extract_fine_types_for_entities(
            sentence, coarse_entities, coarse_fine_mapping
        )
        
        # Mark source as forward
        for entity in result_entities:
            entity['source'] = 'Ê≠£Âêë'
        
        logger.info(f"  ‚úÖ Ê≠£ÂêëÂÆåÊàêÔºö{len(result_entities)} ‰∏™ÂÆåÊï¥ÂÆû‰Ωì")
        return result_entities
    
    async def _reverse_extraction(self, sentence: str, coarse_fine_mapping: Dict[str, List[str]]) -> List[Dict[str, str]]:
        """
        Reverse extraction: Fine -> Coarse
        Áõ¥Êé•‰ΩøÁî®Êò†Â∞ÑË°®Â∞ÜÁªÜÁ≤íÂ∫¶Á±ªÂûãÊò†Â∞ÑÂà∞Á≤óÁ≤íÂ∫¶Á±ªÂûã
        
        Args:
            sentence: Input sentence
            coarse_fine_mapping: Mapping from coarse to fine types
            
        Returns:
            List of entities with fine and coarse types
        """
        logger.info(f"  ‚¨ÖÔ∏è  ÂèçÂêëÊäΩÂèñÔºöÁªÜÁ≤íÂ∫¶ ‚Üí Á≤óÁ≤íÂ∫¶ÔºàÁõ¥Êé•Êò†Â∞ÑÔºâ")
        
        # Build reverse mapping: fine_type -> coarse_type for fast lookup
        fine_to_coarse = {}
        all_fine_types = []
        
        for coarse_type, fine_types in coarse_fine_mapping.items():
            for fine_type in fine_types:
                fine_type_lower = fine_type.lower()
                # Store the mapping (lowercase for case-insensitive lookup)
                fine_to_coarse[fine_type_lower] = coarse_type
                # Keep original case for extraction
                if fine_type_lower not in [ft.lower() for ft in all_fine_types]:
                    all_fine_types.append(fine_type)
        
        logger.info(f"  üìã ÂèØÁî®ÁªÜÁ≤íÂ∫¶Á±ªÂûãÊï∞Èáè: {len(all_fine_types)}")
        logger.info(f"  üìä ÊûÑÂª∫ÂèçÂêëÊò†Â∞ÑË°®: {len(fine_to_coarse)} ‰∏™ÁªÜÁ≤íÂ∫¶‚ÜíÁ≤óÁ≤íÂ∫¶Êò†Â∞Ñ")
        
        # Extract entities with fine types directly
        fine_entities = await self._extract_fine_entities_directly(sentence, all_fine_types)
        
        if not fine_entities:
            logger.warning(f"    Êú™ÊèêÂèñÂà∞ÂÆû‰ΩìÔºàÂèçÂêëÔºâ")
            return []
        
        logger.info(f"  ‚úÖ ÂèçÂêëÊèêÂèñÂà∞ {len(fine_entities)} ‰∏™ÁªÜÁ≤íÂ∫¶ÂÆû‰Ωì")
        for entity in fine_entities:
            logger.info(f"     - {entity['name']} -> {entity['fine_type']}")
        
        # Use the reverse mapping to find coarse types (EXACT MATCH ONLY)
        result_entities = []
        for entity in fine_entities:
            fine_type = entity['fine_type']
            fine_type_lower = fine_type.lower()
            
            # Direct lookup in reverse mapping - EXACT MATCH ONLY
            coarse_type = fine_to_coarse.get(fine_type_lower)
            
            if coarse_type:
                result_entities.append({
                    'name': entity['name'],
                    'coarse_type': coarse_type,
                    'fine_type': fine_type,
                    'description': entity.get('description', ''),
                    'source': 'ÂèçÂêë'
                })
                logger.info(f"     ‚úì {entity['name']}: {fine_type} ‚Üí {coarse_type} (Á≤æÁ°ÆÂåπÈÖç)")
            else:
                logger.info(f"     ‚úó {entity['name']}: {fine_type} Êó†Ê≥ïÁ≤æÁ°ÆÊò†Â∞ÑÂà∞Á≤óÁ≤íÂ∫¶Á±ªÂûãÔºåË∑≥Ëøá")
        
        logger.info(f"  ‚úÖ ÂèçÂêëÂÆåÊàêÔºö{len(result_entities)} ‰∏™ÂÆåÊï¥ÂÆû‰ΩìÔºàÁ≤æÁ°ÆÂåπÈÖçÔºâ")
        return result_entities
    
    def _merge_extraction_results(self, forward_entities: List[Dict], reverse_entities: List[Dict], sentence: str) -> List[Dict]:
        """
        Merge results from forward and reverse extraction.
        
        Strategy:
        1. If entity appears in both, prefer the one with more specific fine type
        2. If entity only appears in one, include it
        3. Deduplicate by entity name (case-insensitive)
        
        Args:
            forward_entities: Results from forward extraction
            reverse_entities: Results from reverse extraction
            sentence: Original sentence for reference
            
        Returns:
            Merged list of entities
        """
        logger.info(f"  üîÄ ÂêàÂπ∂Á≠ñÁï•Ôºö‰ºòÂÖàÈÄâÊã©Êõ¥ÂÖ∑‰ΩìÁöÑÁªÜÁ≤íÂ∫¶Á±ªÂûã")
        logger.info(f"  üìä Ê≠£ÂêëÁªìÊûú: {len(forward_entities)} ‰∏™ÂÆû‰Ωì")
        logger.info(f"  üìä ÂèçÂêëÁªìÊûú: {len(reverse_entities)} ‰∏™ÂÆû‰Ωì")
        
        # Build a dictionary keyed by lowercase entity name
        entity_dict = {}
        
        # Add forward entities
        for entity in forward_entities:
            name_lower = entity['name'].lower()
            entity_dict[name_lower] = entity.copy()
        
        # Process reverse entities
        for entity in reverse_entities:
            name_lower = entity['name'].lower()
            
            if name_lower in entity_dict:
                # Entity exists in both - compare and choose better one
                existing = entity_dict[name_lower]
                
                # Prefer entity where coarse and fine types are different
                existing_same = existing['coarse_type'].lower() == existing['fine_type'].lower()
                new_same = entity['coarse_type'].lower() == entity['fine_type'].lower()
                
                if existing_same and not new_same:
                    # New entity has different types, prefer it
                    logger.info(f"  üîÑ ÊõøÊç¢ '{entity['name']}': {existing['source']}ÁªìÊûúÁ≤óÁªÜÁ±ªÂûãÁõ∏ÂêåÔºå‰ΩøÁî®{entity['source']}ÁªìÊûú")
                    entity_dict[name_lower] = entity.copy()
                    entity_dict[name_lower]['source'] = 'ÂèçÂêë(ÊõøÊç¢)'
                elif not existing_same and new_same:
                    # Existing entity is better, keep it
                    logger.info(f"  ‚úì ‰øùÁïô '{entity['name']}': {existing['source']}ÁªìÊûúÊõ¥ÂÖ∑‰Ωì")
                else:
                    # Both are similar, mark as from both sources
                    logger.info(f"  ‚âà ÈáçÂ§ç '{entity['name']}': ‰∏§ÁßçÊñπÊ≥ïÁªìÊûúÁõ∏‰ººÔºå‰øùÁïô{existing['source']}ÁªìÊûú")
                    entity_dict[name_lower]['source'] = 'Ê≠£Âêë+ÂèçÂêë'
            else:
                # New entity only in reverse
                logger.info(f"  ‚ûï Êñ∞Â¢û '{entity['name']}': ‰ªÖÂú®ÂèçÂêëÊäΩÂèñ‰∏≠ÂèëÁé∞")
                entity_dict[name_lower] = entity.copy()
        
        # Convert back to list
        merged_entities = list(entity_dict.values())
        
        logger.info(f"  ‚úÖ ÂêàÂπ∂ÂÆåÊàêÔºöÊÄªËÆ° {len(merged_entities)} ‰∏™ÂîØ‰∏ÄÂÆû‰Ωì")
        logger.info(f"     - Ê≠£ÂêëÁã¨Êúâ: {sum(1 for e in merged_entities if e['source'] == 'Ê≠£Âêë')}")
        logger.info(f"     - ÂèçÂêëÁã¨Êúâ: {sum(1 for e in merged_entities if e['source'] == 'ÂèçÂêë')}")
        logger.info(f"     - ÂèçÂêëÊõøÊç¢: {sum(1 for e in merged_entities if e['source'] == 'ÂèçÂêë(ÊõøÊç¢)')}")
        logger.info(f"     - ‰∏§ËÄÖÈÉΩÊúâ: {sum(1 for e in merged_entities if e['source'] == 'Ê≠£Âêë+ÂèçÂêë')}")
        
        return merged_entities
    
    async def _extract_coarse_entities(self, sentence: str, coarse_types: List[str], max_gleaning: int = 1) -> List[Dict]:
        """
        Stage 1: Extract entities and classify them into coarse types with gleaning.
        
        Args:
            sentence: The text to extract entities from
            coarse_types: List of available coarse types
            max_gleaning: Maximum number of gleaning iterations to find missed entities
            
        Returns:
            List of entities with name and coarse_type
        """
        from core.prompts import PROMPTS
        
        # Get delimiters from prompts
        tuple_delimiter = PROMPTS["DEFAULT_TUPLE_DELIMITER"]
        completion_delimiter = PROMPTS["DEFAULT_COMPLETION_DELIMITER"]
        
        # Add "other/ÂÖ∂‰ªñ" type for misannotated data
        coarse_types_with_other = coarse_types + ["other", "ÂÖ∂‰ªñ"]
        
        # Build coarse extraction prompt
        coarse_types_str = ", ".join(coarse_types_with_other)
        
        # Select examples based on language environment variable
        if self.language.lower() == "chinese":
            examples = HIERARCHICAL_PROMPTS.get("coarse_extraction_examples_zh", 
                                               HIERARCHICAL_PROMPTS["coarse_extraction_examples_en"])
        else:
            examples = HIERARCHICAL_PROMPTS["coarse_extraction_examples_en"]
        examples_str = "\n".join(examples)
        
        # Initial extraction
        system_prompt = HIERARCHICAL_PROMPTS["coarse_extraction_system_prompt"].format(
            entity_types=coarse_types_str,
            tuple_delimiter=tuple_delimiter,
            completion_delimiter=completion_delimiter,
            language=self.language,
            examples=examples_str,
            input_text=sentence
        )
        
        # Call LLM
        response = await self.llm_func(system_prompt)
        
        # Parse response
        entities = self._parse_coarse_entities_response(response, coarse_types_with_other, tuple_delimiter, completion_delimiter)
        
        # Gleaning: Continue extraction for missed entities
        for gleaning_round in range(max_gleaning):
            if not entities:
                break
                
            logger.info(f"    üîÑ ÁªßÁª≠ÊäΩÂèñÔºàÁ¨¨ {gleaning_round + 1}/{max_gleaning} ËΩÆÔºâÊ£ÄÊü•ÈÅóÊºèÂÆû‰Ωì...")
            
            # Build continue extraction prompt
            extracted_names = [e["name"] for e in entities]
            continue_prompt = self._build_coarse_continue_prompt(
                sentence, 
                coarse_types_str, 
                extracted_names,
                tuple_delimiter,
                completion_delimiter
            )
            
            # Call LLM for continue extraction
            continue_response = await self.llm_func(continue_prompt)
            
            # Parse continue response
            new_entities = self._parse_coarse_entities_response(
                continue_response, 
                coarse_types_with_other, 
                tuple_delimiter, 
                completion_delimiter
            )
            
            # Add new entities (avoid duplicates)
            existing_names_lower = {e["name"].lower() for e in entities}
            added_count = 0
            for entity in new_entities:
                if entity["name"].lower() not in existing_names_lower:
                    entities.append(entity)
                    existing_names_lower.add(entity["name"].lower())
                    added_count += 1
            
            if added_count > 0:
                logger.info(f"    ‚úÖ ÂèëÁé∞ {added_count} ‰∏™ÈÅóÊºèÂÆû‰Ωì")
            else:
                logger.info(f"    ‚úì Êú™ÂèëÁé∞ÈÅóÊºèÂÆû‰ΩìÔºåÂÅúÊ≠¢ÁªßÁª≠ÊäΩÂèñ")
                break
        
        return entities
    
    def _parse_coarse_entities_response(
        self, 
        response: str, 
        coarse_types: List[str], 
        tuple_delimiter: str, 
        completion_delimiter: str
    ) -> List[Dict]:
        """Parse coarse entities from LLM response."""
        entities = []
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line == completion_delimiter:
                continue
            
            # Parse: entity<|>name<|>type<|>description
            if tuple_delimiter in line:
                parts = line.split(tuple_delimiter)
                if len(parts) >= 3 and parts[0].strip().lower() == "entity":
                    entity_name = parts[1].strip()
                    entity_type = parts[2].strip()
                    
                    # Validate coarse type
                    if entity_type.lower() in [ct.lower() for ct in coarse_types]:
                        entities.append({
                            "name": entity_name,
                            "coarse_type": entity_type
                        })
                    else:
                        # Try to map to one of the coarse types
                        mapped_type = self._map_to_coarse_type(entity_type, coarse_types, {})
                        if mapped_type:
                            entities.append({
                                "name": entity_name,
                                "coarse_type": mapped_type
                            })
        
        return entities
    
    def _build_coarse_continue_prompt(
        self, 
        sentence: str, 
        coarse_types_str: str, 
        extracted_names: List[str],
        tuple_delimiter: str,
        completion_delimiter: str
    ) -> str:
        """Build continue extraction prompt for coarse entities."""
        extracted_list = "\n".join([f"- {name}" for name in extracted_names])
        
        # Use standardized prompt from HIERARCHICAL_PROMPTS
        prompt = HIERARCHICAL_PROMPTS["coarse_continue_extraction_prompt"].format(
            extracted_entities_list=extracted_list,
            sentence=sentence,
            coarse_types=coarse_types_str,
            tuple_delimiter=tuple_delimiter,
            completion_delimiter=completion_delimiter
        )
        return prompt
    
    async def _extract_fine_types_for_entities(
        self, 
        sentence: str, 
        coarse_entities: List[Dict], 
        coarse_fine_mapping: Dict[str, List[str]]
    ) -> List[Dict]:
        """
        Stage 2: Extract fine-grained types for each entity based on its coarse type.
        
        Args:
            sentence: The original sentence
            coarse_entities: List of entities with coarse types
            coarse_fine_mapping: Mapping from coarse to fine types
            
        Returns:
            List of entities with both coarse_type and fine_type
        """
        result_entities = []
        
        # Group entities by coarse type for batch processing
        entities_by_coarse = defaultdict(list)
        for entity in coarse_entities:
            coarse_type = entity["coarse_type"]
            entities_by_coarse[coarse_type].append(entity)
        
        # Process each coarse type group
        for coarse_type, entities in entities_by_coarse.items():
            # Filter out entities with "other" or "ÂÖ∂‰ªñ" type (misannotated data)
            if coarse_type.lower() in ["other", "ÂÖ∂‰ªñ"]:
                logger.info(f"\n  ‚ö†Ô∏è  Ë∑≥ËøáÊ†áÊ≥®‰∏∫ '{coarse_type}' ÁöÑÂÆû‰Ωì ({len(entities)} ‰∏™): ÂèØËÉΩ‰∏∫Ê†áÊ≥®ÈîôËØØ")
                for entity in entities:
                    logger.info(f"     - {entity['name']} (Ê†áÊ≥®ÈîôËØØÔºåÂ∑≤ËøáÊª§)")
                continue
            
            logger.info(f"\n  Â§ÑÁêÜÁ≤óÁ≤íÂ∫¶Á±ªÂûã: {coarse_type} ({len(entities)} ‰∏™ÂÆû‰Ωì)")
            
            # Get fine types for this coarse type
            fine_types = coarse_fine_mapping.get(coarse_type, [])
            
            if not fine_types:
                logger.warning(f"    Ê≤°ÊúâÊâæÂà∞ÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºå‰ΩøÁî®Á≤óÁ≤íÂ∫¶Á±ªÂûã")
                for entity in entities:
                    result_entities.append({
                        "name": entity["name"],
                        "coarse_type": coarse_type,
                        "fine_type": coarse_type
                    })
                continue
            
            logger.info(f"  üìã ÂèØÁî®ÁªÜÁ≤íÂ∫¶Á±ªÂûã ({len(fine_types)}): {fine_types[:10]}{'...' if len(fine_types) > 10 else ''}")
            
            # Extract fine types for all entities of this coarse type (with gleaning)
            fine_typed_entities = await self._extract_fine_types_with_gleaning(
                sentence, entities, coarse_type, fine_types, max_gleaning=1
            )
            
            # Add to results
            result_entities.extend(fine_typed_entities)
        
        return result_entities
    
    async def _extract_fine_types_batch(
        self,
        sentence: str,
        entities: List[Dict],
        coarse_type: str,
        fine_types: List[str]
    ) -> List[Dict]:
        """
        ÊâπÈáèÊèêÂèñÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºà‰ΩøÁî®‰∏âÂÖÉÁªÑÊ†ºÂºèÔºåÊïàÁéáÊõ¥È´òÔºâ
        
        Args:
            sentence: The sentence
            entities: List of entities with coarse type
            coarse_type: The coarse type
            fine_types: Available fine types
            
        Returns:
            List of entities with both coarse and fine types
        """
        from core.prompts import PROMPTS
        
        try:
            # Select examples based on language
            if self.language.lower() == "chinese":
                examples = HIERARCHICAL_PROMPTS.get("fine_extraction_batch_examples_zh", [])
            else:
                examples = HIERARCHICAL_PROMPTS.get("fine_extraction_batch_examples_en", [])
            examples_str = "\n".join(examples)
            
            # Get delimiters
            tuple_delimiter = PROMPTS.get("DEFAULT_TUPLE_DELIMITER", "<|#|>")
            completion_delimiter = PROMPTS.get("DEFAULT_COMPLETION_DELIMITER", "<|COMPLETE|>")
            
            # Build entities list
            entities_list = ", ".join([e["name"] for e in entities])
            fine_types_str = ", ".join(fine_types)
            
            # Build batch extraction prompt
            system_prompt = HIERARCHICAL_PROMPTS["fine_extraction_batch_system_prompt"].format(
                sentence=sentence,
                coarse_type=coarse_type,
                entities_list=entities_list,
                fine_types=fine_types_str,
                tuple_delimiter=tuple_delimiter,
                completion_delimiter=completion_delimiter,
                language=self.language,
                examples=examples_str
            )
            
            logger.debug(f"  üîß ‰ΩøÁî®ÊâπÈáèÁªÜÁ≤íÂ∫¶ÊäΩÂèñÔºåÂ§ÑÁêÜ {len(entities)} ‰∏™ÂÆû‰Ωì")
            
            # Call LLM
            response = await self.llm_func(system_prompt)
            
            # Parse response using the standard parser
            from core.extractor import _process_extraction_result
            import time
            
            entities_dict, _ = await _process_extraction_result(
                response,
                chunk_key="fine_batch_extraction",
                timestamp=int(time.time()),
                file_path="fine_batch",
                tuple_delimiter=tuple_delimiter,
                completion_delimiter=completion_delimiter,
                use_hierarchical_types=False
            )
            
            # Map extracted entities back to original entities
            result_entities = []
            entity_name_lower_map = {e["name"].lower(): e["name"] for e in entities}
            
            for entity_name_lower, entity_list in entities_dict.items():
                if entity_list:
                    entity_data = entity_list[0]
                    fine_type = entity_data.get("entity_type", "")
                    
                    # Find original entity name
                    original_name = entity_name_lower_map.get(entity_name_lower.lower())
                    if not original_name:
                        # Try exact match
                        original_name = next((e["name"] for e in entities if e["name"].lower() == entity_name_lower.lower()), entity_name_lower)
                    
                    # Validate fine type
                    if fine_type and fine_type.lower() in [ft.lower() for ft in fine_types]:
                        result_entities.append({
                            "name": original_name,
                            "coarse_type": coarse_type,
                            "fine_type": fine_type
                        })
                        logger.info(f"    ‚úì {original_name}: {coarse_type} -> {fine_type}")
                    else:
                        # Fallback to coarse type
                        result_entities.append({
                            "name": original_name,
                            "coarse_type": coarse_type,
                            "fine_type": coarse_type
                        })
                        logger.warning(f"    ‚ö† {original_name}: Êó†ÊïàÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºå‰ΩøÁî®Á≤óÁ≤íÂ∫¶Á±ªÂûã")
            
            # Handle entities not in response
            extracted_names_lower = {e["name"].lower() for e in result_entities}
            for entity in entities:
                if entity["name"].lower() not in extracted_names_lower:
                    result_entities.append({
                        "name": entity["name"],
                        "coarse_type": coarse_type,
                        "fine_type": coarse_type
                    })
                    logger.warning(f"    ‚ö† {entity['name']}: Êú™ÊèêÂèñÂà∞Ôºå‰ΩøÁî®Á≤óÁ≤íÂ∫¶Á±ªÂûã")
            
            return result_entities
            
        except Exception as e:
            logger.error(f"  ‚ùå ÊâπÈáèÁªÜÁ≤íÂ∫¶ÊäΩÂèñÂ§±Ë¥•: {e}")
            # Fallback to individual extraction
            logger.info(f"  üîÑ ÂõûÈÄÄÂà∞ÈÄê‰∏™ÊäΩÂèñÊñπÂºè...")
            return await self._extract_fine_types_with_gleaning_individual(
                sentence, entities, coarse_type, fine_types, max_gleaning=0
            )
    
    async def _extract_fine_types_with_gleaning(
        self,
        sentence: str,
        entities: List[Dict],
        coarse_type: str,
        fine_types: List[str],
        max_gleaning: int = 1
    ) -> List[Dict]:
        """
        Extract fine types for entities with gleaning to catch missed entities.
        ‰ºòÂÖà‰ΩøÁî®ÊâπÈáèÊäΩÂèñÔºåÂ¶ÇÊûúÂÆû‰ΩìÊï∞ÈáèËæÉÂ∞ëÂàô‰ΩøÁî®ÈÄê‰∏™ÊäΩÂèñ„ÄÇ
        
        Args:
            sentence: The sentence
            entities: List of entities with coarse type
            coarse_type: The coarse type
            fine_types: Available fine types
            max_gleaning: Maximum gleaning iterations
            
        Returns:
            List of entities with both coarse and fine types
        """
        # Â¶ÇÊûúÂÆû‰ΩìÊï∞Èáè>=3Ôºå‰ΩøÁî®ÊâπÈáèÊäΩÂèñ
        if len(entities) >= 3:
            logger.info(f"    üì¶ ‰ΩøÁî®ÊâπÈáèÊäΩÂèñÊ®°Âºè ({len(entities)} ‰∏™ÂÆû‰Ωì)")
            result_entities = await self._extract_fine_types_batch(
                sentence, entities, coarse_type, fine_types
            )
        else:
            logger.info(f"    üîç ‰ΩøÁî®ÈÄê‰∏™ÊäΩÂèñÊ®°Âºè ({len(entities)} ‰∏™ÂÆû‰Ωì)")
            result_entities = await self._extract_fine_types_with_gleaning_individual(
                sentence, entities, coarse_type, fine_types, max_gleaning
            )
        
        return result_entities
    
    async def _extract_fine_types_with_gleaning_individual(
        self,
        sentence: str,
        entities: List[Dict],
        coarse_type: str,
        fine_types: List[str],
        max_gleaning: int = 1
    ) -> List[Dict]:
        """
        ÈÄê‰∏™ÊèêÂèñÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºàÂéüÊñπÊ≥ïÔºåÁî®‰∫éÂ∞ëÈáèÂÆû‰ΩìÊàñÊâπÈáèÊäΩÂèñÂ§±Ë¥•Êó∂ÁöÑÂõûÈÄÄÔºâ
        
        Args:
            sentence: The sentence
            entities: List of entities with coarse type
            coarse_type: The coarse type
            fine_types: Available fine types
            max_gleaning: Maximum gleaning iterations
            
        Returns:
            List of entities with both coarse and fine types
        """
        result_entities = []
        
        # Initial extraction for all entities
        for entity in entities:
            entity_name = entity["name"]
            fine_type = await self._extract_fine_type_for_entity(
                sentence, entity_name, coarse_type, fine_types
            )
            
            result_entities.append({
                "name": entity_name,
                "coarse_type": coarse_type,
                "fine_type": fine_type if fine_type else coarse_type
            })
            
            logger.info(f"    ‚úì {entity_name}: {coarse_type} -> {fine_type if fine_type else coarse_type}")
        
        # Gleaning: Check for missed entities of this coarse type
        for gleaning_round in range(max_gleaning):
            logger.info(f"    üîÑ ÁªßÁª≠ÊäΩÂèñÔºàÁ¨¨ {gleaning_round + 1}/{max_gleaning} ËΩÆÔºâÊ£ÄÊü•ÈÅóÊºèÁöÑ {coarse_type} Á±ªÂûãÂÆû‰Ωì...")
            
            # Build continue extraction prompt for this coarse type
            extracted_names = [e["name"] for e in result_entities]
            continue_prompt = self._build_fine_continue_prompt(
                sentence,
                coarse_type,
                fine_types,
                extracted_names
            )
            
            # Call LLM for continue extraction
            continue_response = await self.llm_func(continue_prompt)
            
            # Parse new entities
            new_entity_names = self._parse_fine_continue_response(continue_response, extracted_names)
            
            if not new_entity_names:
                logger.info(f"    ‚úì Êú™ÂèëÁé∞ÈÅóÊºèÁöÑ {coarse_type} ÂÆû‰ΩìÔºåÂÅúÊ≠¢ÁªßÁª≠ÊäΩÂèñ")
                break
            
            logger.info(f"    ‚úÖ ÂèëÁé∞ {len(new_entity_names)} ‰∏™ÈÅóÊºèÁöÑ {coarse_type} ÂÆû‰Ωì")
            
            # Extract fine types for new entities
            for entity_name in new_entity_names:
                fine_type = await self._extract_fine_type_for_entity(
                    sentence, entity_name, coarse_type, fine_types
                )
                
                result_entities.append({
                    "name": entity_name,
                    "coarse_type": coarse_type,
                    "fine_type": fine_type if fine_type else coarse_type
                })
                
                logger.info(f"    ‚úì (Ë°•ÂÖÖ) {entity_name}: {coarse_type} -> {fine_type if fine_type else coarse_type}")
        
        return result_entities
    
    def _build_fine_continue_prompt(
        self,
        sentence: str,
        coarse_type: str,
        fine_types: List[str],
        extracted_names: List[str]
    ) -> str:
        """Build continue extraction prompt for fine-grained entities."""
        extracted_list = "\n".join([f"- {name}" for name in extracted_names])
        fine_types_str = ", ".join(fine_types)  # No limit on types
        
        # Use standardized prompt from HIERARCHICAL_PROMPTS
        prompt = HIERARCHICAL_PROMPTS["fine_continue_extraction_prompt"].format(
            coarse_type=coarse_type,
            extracted_entities_list=extracted_list,
            sentence=sentence,
            fine_types=fine_types_str
        )
        return prompt
    
    def _parse_fine_continue_response(self, response: str, existing_names: List[str]) -> List[str]:
        """Parse continue extraction response for fine entities."""
        response = response.strip()
        
        if not response or "NONE" in response.upper():
            return []
        
        # Parse entity names from response
        new_names = []
        existing_names_lower = {name.lower() for name in existing_names}
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Remove leading markers like "- ", "* ", numbers, etc.
            entity_name = line.lstrip('-*‚Ä¢123456789. ').strip()
            
            if entity_name and entity_name.lower() not in existing_names_lower:
                new_names.append(entity_name)
                existing_names_lower.add(entity_name.lower())
        
        return new_names
    
    async def _extract_fine_type_for_entity(
        self, 
        sentence: str, 
        entity_name: str, 
        coarse_type: str, 
        fine_types: List[str],
        max_retries: int = 2
    ) -> str:
        """
        Extract fine-grained type for a single entity using triplet format.
        If the extracted fine type is the same as coarse type, add error to history and retry extraction.
        
        Args:
            sentence: The sentence containing the entity
            entity_name: Name of the entity
            coarse_type: Coarse type of the entity
            fine_types: List of available fine types for this coarse type
            max_retries: Maximum number of retries if coarse and fine types are the same
            
        Returns:
            Fine-grained type or None if extraction fails
        """
        from core.prompts import PROMPTS
        
        history_messages = []  # Track conversation history for retries
        
        # Get delimiters
        tuple_delimiter = PROMPTS.get("DEFAULT_TUPLE_DELIMITER", "<|#|>")
        completion_delimiter = PROMPTS.get("DEFAULT_COMPLETION_DELIMITER", "<|COMPLETE|>")
        
        for attempt in range(max_retries + 1):
            try:
                # Get examples for this coarse type based on language
                if self.language.lower() == "chinese":
                    examples_dict = HIERARCHICAL_PROMPTS.get("fine_extraction_examples_zh", 
                                                            HIERARCHICAL_PROMPTS["fine_extraction_examples"])
                else:
                    examples_dict = HIERARCHICAL_PROMPTS.get("fine_extraction_examples_en", 
                                                            HIERARCHICAL_PROMPTS["fine_extraction_examples"])
                
                examples = examples_dict.get(
                    coarse_type.lower(),
                    examples_dict.get("person", examples_dict.get("‰∫∫", ""))
                )
                
                # Build fine extraction prompt (no limit on types)
                fine_types_str = ", ".join(fine_types)
                
                prompt = HIERARCHICAL_PROMPTS["fine_extraction_system_prompt"].format(
                    entity_name=entity_name,
                    coarse_type=coarse_type,
                    sentence=sentence,
                    fine_types=fine_types_str,
                    examples=examples,
                    tuple_delimiter=tuple_delimiter,
                    completion_delimiter=completion_delimiter,
                    language=self.language
                )
                
                # Call LLM with history messages on retry
                if history_messages:
                    # When using history, the complete() function ignores the prompt parameter
                    # The new prompt should already be in the history_messages
                    response = await self.llm_func("", history_messages=history_messages)
                else:
                    response = await self.llm_func(prompt)
                
                # Parse triplet response
                from core.extractor import _process_extraction_result
                import time
                
                entities_dict, _ = await _process_extraction_result(
                    response,
                    chunk_key="fine_single_extraction",
                    timestamp=int(time.time()),
                    file_path="fine_single",
                    tuple_delimiter=tuple_delimiter,
                    completion_delimiter=completion_delimiter,
                    use_hierarchical_types=False
                )
                
                # Extract fine type from parsed entities
                extracted_type = None
                for entity_name_lower, entity_list in entities_dict.items():
                    if entity_list and entity_name.lower() in entity_name_lower.lower():
                        extracted_type = entity_list[0].get("entity_type", "")
                        break
                
                # If not found by name match, try to get any extracted entity (should be only one)
                if not extracted_type and entities_dict:
                    for entity_list in entities_dict.values():
                        if entity_list:
                            extracted_type = entity_list[0].get("entity_type", "")
                            break
                
                # Check if extracted type is the same as coarse type
                if extracted_type and extracted_type.lower() != coarse_type.lower():
                    # Success: fine type is different from coarse type
                    return extracted_type
                elif extracted_type and extracted_type.lower() == coarse_type.lower():
                    # Fine type is same as coarse type, need to retry
                    if attempt < max_retries:
                        logger.warning(f"      ÁªÜÁ≤íÂ∫¶Á±ªÂûã‰∏éÁ≤óÁ≤íÂ∫¶Á±ªÂûãÁõ∏Âêå ({extracted_type}), Â∞ÜÈîôËØØÊ∑ªÂä†Âà∞ÂéÜÂè≤Âπ∂ÈáçÊñ∞ÊäΩÂèñ (Â∞ùËØï {attempt + 1}/{max_retries})...")
                        
                        # Add the error to history for next attempt
                        error_message = f"""‚ùå ERROR: Your previous answer "{extracted_type}" is INCORRECT because it is the same as the coarse type "{coarse_type}".

üî¥ CRITICAL REQUIREMENT: You MUST provide a MORE SPECIFIC fine-grained type from the available types.

Available fine-grained types: {fine_types_str}

Please provide a MORE SPECIFIC type that is DIFFERENT from "{coarse_type}" in the triplet format:
entity{tuple_delimiter}{entity_name}{tuple_delimiter}[fine_type]{tuple_delimiter}[description]
{completion_delimiter}"""
                        
                        # Build history messages in the format expected by OpenAI API
                        if not history_messages:
                            # First retry: add initial user message and assistant's wrong response
                            history_messages.append({"role": "user", "content": prompt})
                            history_messages.append({"role": "assistant", "content": response})
                        
                        # Add error feedback as user message
                        history_messages.append({"role": "user", "content": error_message})
                        
                        continue
                    else:
                        logger.warning(f"      Â§öÊ¨°Â∞ùËØïÂêé‰ªç‰∏∫Áõ∏ÂêåÁ±ªÂûãÔºå‰øùÊåÅÁ≤óÁ≤íÂ∫¶Á±ªÂûã")
                        return coarse_type
                else:
                    # No valid type extracted
                    if attempt < max_retries:
                        logger.warning(f"      Êú™ÊèêÂèñÂà∞ÊúâÊïàÁ±ªÂûã, Â∞ÜÈîôËØØÊ∑ªÂä†Âà∞ÂéÜÂè≤Âπ∂ÈáçËØï (Â∞ùËØï {attempt + 1}/{max_retries})...")
                        
                        # Add the error to history
                        error_message = f"""‚ùå ERROR: Your previous response did not contain a valid triplet format.

Available fine-grained types: {fine_types_str}

Please provide the fine-grained type in the triplet format:
entity{tuple_delimiter}{entity_name}{tuple_delimiter}[fine_type]{tuple_delimiter}[description]
{completion_delimiter}"""
                        
                        if not history_messages:
                            history_messages.append({"role": "user", "content": prompt})
                            history_messages.append({"role": "assistant", "content": response})
                        
                        history_messages.append({"role": "user", "content": error_message})
                        continue
                    else:
                        logger.warning(f"      Êó†Ê≥ïÊèêÂèñÊúâÊïàÁªÜÁ≤íÂ∫¶Á±ªÂûãÔºå‰ΩøÁî®Á≤óÁ≤íÂ∫¶Á±ªÂûã")
                        return coarse_type
                
            except Exception as e:
                logger.info(f"    ‚ùå ÊèêÂèñÁªÜÁ≤íÂ∫¶Á±ªÂûãÂ§±Ë¥• (Â∞ùËØï {attempt + 1}): {e}")
                if attempt >= max_retries:
                    return coarse_type
        
        return coarse_type
    
    async def _extract_fine_entities_directly(self, sentence: str, fine_types: List[str], max_gleaning: int = 1) -> List[Dict]:
        """
        Extract entities directly with fine-grained types (for reverse extraction).
        
        Args:
            sentence: The text to extract entities from
            fine_types: List of all available fine-grained types
            max_gleaning: Maximum number of gleaning iterations
            
        Returns:
            List of entities with name and fine_type
        """
        try:
            from core.prompts import PROMPTS
            
            # Select examples based on language
            if self.language.lower() == "chinese":
                examples = HIERARCHICAL_PROMPTS.get("reverse_extraction_examples_zh", [])
            else:
                examples = HIERARCHICAL_PROMPTS.get("reverse_extraction_examples_en", [])
            examples_str = "\n".join(examples)
            
            # Get delimiters
            tuple_delimiter = PROMPTS.get("DEFAULT_TUPLE_DELIMITER", "<|#|>")
            completion_delimiter = PROMPTS.get("DEFAULT_COMPLETION_DELIMITER", "<|COMPLETE|>")
            
            # Build reverse extraction prompt with examples
            fine_types_str = ", ".join(fine_types)  # Use all fine types without limit
            system_prompt = HIERARCHICAL_PROMPTS["reverse_extraction_system_prompt"].format(
                entity_types=fine_types_str,
                tuple_delimiter=tuple_delimiter,
                completion_delimiter=completion_delimiter,
                language=self.language,
                examples=examples_str,
                input_text=sentence
            )
            
            # Call LLM with optimized prompt
            logger.debug(f"  üîß ‰ΩøÁî®‰ºòÂåñÁöÑÂèçÂêëÊäΩÂèñÊèêÁ§∫ËØçÔºåÂåÖÂê´ {len(fine_types)} ‰∏™ÁªÜÁ≤íÂ∫¶Á±ªÂûã")
            response = await self.llm_func(
                "",  # Empty user prompt since system prompt contains everything
                system_prompt=system_prompt
            )
            
            # Parse response
            from core.extractor import _process_extraction_result
            import time
            
            entities_dict, _ = await _process_extraction_result(
                response,
                chunk_key="reverse_extraction",
                timestamp=int(time.time()),
                file_path="reverse",
                tuple_delimiter=tuple_delimiter,
                completion_delimiter=completion_delimiter,
                use_hierarchical_types=False
            )
            
            # Convert to our format
            entities = []
            for entity_name, entity_list in entities_dict.items():
                if entity_list:
                    entity_data = entity_list[0]
                    entity_type = entity_data.get("entity_type", "")
                    description = entity_data.get("description", "")
                    
                    entities.append({
                        'name': entity_name,
                        'fine_type': entity_type,
                        'description': description
                    })
            
            return entities
            
        except Exception as e:
            logger.error(f"    ‚ùå ÂèçÂêëÊäΩÂèñÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞ÊóßÊñπÊ≥ï: {e}")
            # Fallback to old method
            try:
                entities_dict, relationships_list = await extract_from_text(
                    text=sentence,
                    entity_types=fine_types,
                    llm_func=self.llm_func,
                    use_hierarchical_types=False,
                    max_gleaning=max_gleaning,
                    language=self.language
                )
                
                entities = []
                for entity_name, entity_list in entities_dict.items():
                    if entity_list:
                        entity_data = entity_list[0]
                        entity_type = entity_data.get("entity_type", "")
                        description = entity_data.get("description", "")
                        
                        entities.append({
                            'name': entity_name,
                            'fine_type': entity_type,
                            'description': description
                        })
                
                return entities
            except Exception as e2:
                logger.error(f"    ‚ùå ÂõûÈÄÄÊñπÊ≥ï‰πüÂ§±Ë¥•: {e2}")
                return []
    
    def _parse_type_from_response(self, response: str, valid_types: List[str]) -> str:
        """
        Parse entity type from LLM response using EXACT matching only.
        Unified parser for all type extraction scenarios.
        
        Args:
            response: LLM response
            valid_types: List of valid types to match against
            
        Returns:
            Matched type or None
        """
        response = response.strip()
        
        # Try to extract from "Type: xxx" or similar patterns
        patterns = ["type:", "answer:", "Á±ªÂûã:", "Á≠îÊ°à:"]
        extracted_text = response.lower()
        
        for pattern in patterns:
            if pattern in extracted_text:
                extracted_text = extracted_text.split(pattern)[-1].strip()
                break
        
        # Remove common punctuation
        extracted_text = extracted_text.replace(".", "").replace(",", "").replace("!", "").replace(";", "").strip()
        
        # Extract first word if multiple words
        first_word = extracted_text.split()[0] if extracted_text.split() else extracted_text
        
        # Try exact match with first word
        for valid_type in valid_types:
            if first_word == valid_type.lower():
                return valid_type
        
        # Try exact match with full extracted text
        for valid_type in valid_types:
            if extracted_text == valid_type.lower():
                return valid_type
        
        # Try exact match with original response
        response_lower = response.lower()
        for valid_type in valid_types:
            if response_lower == valid_type.lower():
                return valid_type
        
        # Try exact match with first line
        first_line = response.split('\n')[0].strip().lower()
        for valid_type in valid_types:
            if first_line == valid_type.lower():
                return valid_type
        
        return None
    
    def _map_to_coarse_type(self, extracted_type: str, available_coarse_types: List[str], coarse_fine_mapping: Dict[str, List[str]] = None) -> str:
        """
        Map an extracted fine type to one of the available coarse types using EXACT matching only.
        No fuzzy or partial matching is performed.
        
        Args:
            extracted_type: The extracted entity type
            available_coarse_types: Available coarse types for this sentence
            coarse_fine_mapping: Coarse-fine mapping for this sentence
            
        Returns:
            Mapped coarse type or None if no exact match found
        """
        extracted_type_lower = extracted_type.lower()
        
        # Check if the extracted type is already a coarse type (exact match)
        if extracted_type_lower in [ct.lower() for ct in available_coarse_types]:
            return extracted_type
        
        # Use the provided mapping if available (exact match only)
        if coarse_fine_mapping:
            for coarse_type, fine_types in coarse_fine_mapping.items():
                if extracted_type_lower in [ft.lower() for ft in fine_types]:
                    return coarse_type
        
        # Try to find a mapping through the type manager (exact match only)
        for coarse_type in available_coarse_types:
            if coarse_type in self.type_manager.coarse_fine_mapping:
                fine_types = self.type_manager.coarse_fine_mapping[coarse_type]
                if extracted_type_lower in [ft.lower() for ft in fine_types]:
                    return coarse_type
        
        # No exact match found
        return None
    
    def show_type_mapping_stats(self, coarse_types: List[str]):
        """Show statistics about the type mapping for given coarse types."""
        logger.info(f"\nüìä Á±ªÂûãÊò†Â∞ÑÁªüËÆ°:")
        logger.info(f"   Á≤óÁ≤íÂ∫¶Á±ªÂûãÊï∞Èáè: {len(coarse_types)}")
        
        total_fine_types = 0
        for coarse_type in coarse_types:
            if coarse_type in self.type_manager.coarse_fine_mapping:
                fine_count = len(self.type_manager.coarse_fine_mapping[coarse_type])
                total_fine_types += fine_count
                logger.info(f"   {coarse_type}: {fine_count} ‰∏™ÁªÜÁ≤íÂ∫¶Á±ªÂûã")
            else:
                logger.info(f"   {coarse_type}: Êú™ÊâæÂà∞Êò†Â∞Ñ")
        
        logger.info(f"   ÊÄªÁªÜÁ≤íÂ∫¶Á±ªÂûãÊï∞Èáè: {total_fine_types}")


def jsonl_to_json(jsonl_file: str, json_file: str, indent: int = 4):
    """
    Convert JSON Lines file to standard JSON format.
    
    Args:
        jsonl_file: Path to input JSONL file
        json_file: Path to output JSON file
        indent: Indentation level for pretty printing
    """
    logger.info(f"üîÑ ËΩ¨Êç¢ JSONL Âà∞ JSON: {jsonl_file} -> {json_file}")
    
    results = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:  # Skip empty lines
                results.append(json.loads(line))
    
    # Save as standard JSON
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=indent)
    
    logger.info(f"‚úÖ ËΩ¨Êç¢ÂÆåÊàêÔºåÂÖ± {len(results)} Êù°ËÆ∞ÂΩï")
    return len(results)


async def process_dataset(input_file: str, output_file: str, max_samples: int = None):
    """
    Process the dataset and generate submission file using JSON Lines format.
    
    Args:
        input_file: Path to input JSON file
        output_file: Path to output JSON file (will be converted from JSONL at the end)
        max_samples: Maximum number of samples to process (for testing)
    """
    logger.info(f"üöÄ ÂºÄÂßãÂ§ÑÁêÜÊï∞ÊçÆÈõÜ: {input_file}")
    
    # Load dataset
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if max_samples:
        data = data[:max_samples]
        logger.info(f"üìä Â§ÑÁêÜÂâç {max_samples} ‰∏™Ê†∑Êú¨")
    
    # Initialize extractor
    extractor = HierarchicalEntityExtractor()
    
    # JSONL temporary file
    jsonl_file = output_file.replace('.json', '.jsonl')
    
    # Clear/create JSONL file (overwrite mode at start)
    os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)
    with open(jsonl_file, 'w', encoding='utf-8') as f:
        pass  # Create empty file
    
    total_samples = len(data)
    processed_count = 0
    
    # Show type mapping statistics for the first sample
    if data:
        first_sample = data[0]
        logger.info(f"\nüìä Á¨¨‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÁ±ªÂûãÊò†Â∞ÑÁªüËÆ°:")
        extractor.show_type_mapping_stats(first_sample['coarse_types'])
    
    for i, sample in enumerate(data, 1):
        # print(f"\nüìù Â§ÑÁêÜÊ†∑Êú¨ {i}/{total_samples}: {sample['id']}")
        
        sentence = sample['sentence']
        coarse_types = sample['coarse_types']
        
        # Show type mapping for this sample (only for first few samples)
        if i <= 3:
            extractor.show_type_mapping_stats(coarse_types)
        
        # Extract entities
        entities = await extractor.extract_entities_for_sentence(sentence, coarse_types)
        
        # Clean entities: remove debug fields like 'source'
        cleaned_entities = []
        for entity in entities:
            cleaned_entity = {
                'name': entity['name'],
                'coarse_type': entity['coarse_type'],
                'fine_type': entity['fine_type']
            }
            # Optionally include description if present
            if 'description' in entity and entity['description']:
                cleaned_entity['description'] = entity['description']
            cleaned_entities.append(cleaned_entity)
        
        # Create result entry with id
        # Preserve original id or generate new one
        sample_id = sample.get('id', str(uuid.uuid4()))
        
        result_entry = {
            "id": sample_id,
            "sentence": sentence,
            "entities": cleaned_entities
        }
        
        # Append to JSONL file (one line per result)
        with open(jsonl_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result_entry, ensure_ascii=False) + '\n')
        
        processed_count += 1
        
        # Print progress
        if i % 10 == 0:
            logger.info(f"üìà Â∑≤Â§ÑÁêÜ {i}/{total_samples} ‰∏™Ê†∑Êú¨")
    
    logger.info(f"\n‚úÖ Â§ÑÁêÜÂÆåÊàê! JSONL Êñá‰ª∂: {jsonl_file}")
    logger.info(f"üìä ÊÄªÂÖ±Â§ÑÁêÜ‰∫Ü {processed_count} ‰∏™Ê†∑Êú¨")
    
    # Convert JSONL to standard JSON
    logger.info(f"\nüîÑ ËΩ¨Êç¢‰∏∫Ê†áÂáÜ JSON Ê†ºÂºè...")
    total_records = jsonl_to_json(jsonl_file, output_file, indent=4)
    
    logger.info(f"‚úÖ ÊúÄÁªàÁªìÊûú‰øùÂ≠òÂà∞: {output_file}")
    
    # Calculate statistics from JSONL file
    total_entities = 0
    sample_results = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                result = json.loads(line)
                total_entities += len(result['entities'])
                if line_num <= 3:
                    sample_results.append(result)
    
    logger.info(f"üìä ÊÄªÂÖ±ÊèêÂèñ‰∫Ü {total_entities} ‰∏™ÂÆû‰Ωì")
    
    # Show some examples
    logger.info(f"\nüìã Á§∫‰æãÁªìÊûú:")
    for i, result in enumerate(sample_results):
        logger.info(f"  Ê†∑Êú¨ {i+1}: {len(result['entities'])} ‰∏™ÂÆû‰Ωì")
        for entity in result['entities'][:2]:  # Show first 2 entities
            logger.info(f"    - {entity['name']}: {entity['coarse_type']} -> {entity['fine_type']}")


async def main():
    """Main function to run the extraction process."""
    input_file = INPUT_FILE
    output_file = OUTPUT_FILE
    
    # For testing, process only first 10 samples
    # For full processing, set max_samples=None
    await process_dataset(input_file, output_file, max_samples=3)
    
    logger.info(f"\nüéâ ÂÆåÊàê! Ê£ÄÊü•ÁªìÊûúÊñá‰ª∂: {output_file}")


async def process_full_dataset():
    """Process the entire dataset (use with caution - will take a long time)."""
    input_file = INPUT_FILE
    output_file = OUTPUT_FILE
    
    print("‚ö†Ô∏è  Ë≠¶Âëä: ËøôÂ∞ÜÂ§ÑÁêÜÊï¥‰∏™Êï∞ÊçÆÈõÜÔºåÂèØËÉΩÈúÄË¶ÅÂæàÈïøÊó∂Èó¥...")
    response = input("ÊòØÂê¶ÁªßÁª≠? (y/N): ")
    
    if response.lower() == 'y':
        await process_dataset(input_file, output_file, max_samples=None)
        logger.info(f"\nüéâ ÂÆåÊï¥Êï∞ÊçÆÈõÜÂ§ÑÁêÜÂÆåÊàê! Ê£ÄÊü•ÁªìÊûúÊñá‰ª∂: {output_file}")
    else:
        print("‚ùå Â∑≤ÂèñÊ∂àÂ§ÑÁêÜ")


if __name__ == "__main__":
    asyncio.run(main())
