"""
å¤šçº¿ç¨‹ç‰ˆæœ¬çš„å±‚çº§åŒ–å®ä½“æŠ½å–
ä½¿ç”¨ asyncio å¹¶å‘å¤„ç†å¤šä¸ªæ ·æœ¬ä»¥æå‡é€Ÿåº¦
"""
import asyncio
import json
import os
import sys
import time
import logging
import uuid
from typing import Dict, List, Any, Optional
from collections import defaultdict
from datetime import datetime
from logging.handlers import RotatingFileHandler
from dotenv import load_dotenv

# åŠ è½½ .env é…ç½®æ–‡ä»¶
load_dotenv()

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api-inference.modelscope.cn/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "Qwen/Qwen3-235B-A22B-Instruct-2507")
EXTRACT_LANGUAGE = os.getenv("EXTRACT_LANGUAGE", "Chinese")

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
os.environ["OPENAI_API_BASE"] = OPENAI_API_BASE
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_MODEL"] = OPENAI_MODEL
os.environ["EXTRACT_LANGUAGE"] = EXTRACT_LANGUAGE

# æ–‡ä»¶è·¯å¾„é…ç½®ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„ï¼‰
INPUT_FILE = os.path.join(PROJECT_ROOT, os.getenv("INPUT_FILE", "data/zh_data_dev1.json"))
OUTPUT_FILE = os.path.join(PROJECT_ROOT, os.getenv("OUTPUT_FILE", "output/submit_results_concurrent.json"))
TYPE_DICT_PATH = os.path.join(PROJECT_ROOT, os.getenv("TYPE_DICT_PATH", "data/coarse_fine_type_dict.json"))

# å¹¶å‘é…ç½®
MAX_CONCURRENT_TASKS = int(os.getenv("MAX_CONCURRENT_TASKS", "2"))
RETRY_TIMES = int(os.getenv("RETRY_TIMES", "2"))
RETRY_DELAY = float(os.getenv("RETRY_DELAY", "3"))
REQUEST_DELAY = float(os.getenv("REQUEST_DELAY", "0.5"))

# æ—¥å¿—é…ç½®
LOG_DIR = os.path.join(PROJECT_ROOT, os.getenv("LOG_DIR", "logs"))
LOG_FILE_MAX_BYTES = int(os.getenv("LOG_FILE_MAX_BYTES", str(10 * 1024 * 1024)))
LOG_FILE_BACKUP_COUNT = int(os.getenv("LOG_FILE_BACKUP_COUNT", "5"))


def setup_logger(name: str = "concurrent_extractor", log_level: int = logging.INFO) -> logging.Logger:
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        name: Loggeråç§°
        log_level: æ—¥å¿—çº§åˆ«
        
    Returns:
        é…ç½®å¥½çš„loggerå¯¹è±¡
    """
    # åˆ›å»ºlogsç›®å½•
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # åˆ›å»ºlogger
    logger = logging.getLogger(name)
    logger.setLevel(log_level)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶handler - è¯¦ç»†æ—¥å¿—ï¼ˆå¸¦è‡ªåŠ¨è½®è½¬ï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    detail_log_file = os.path.join(LOG_DIR, f'concurrent_extraction_{timestamp}.log')
    file_handler = RotatingFileHandler(
        detail_log_file,
        maxBytes=LOG_FILE_MAX_BYTES,
        backupCount=LOG_FILE_BACKUP_COUNT,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # é”™è¯¯æ—¥å¿—æ–‡ä»¶handler
    error_log_file = os.path.join(LOG_DIR, f'concurrent_extraction_errors_{timestamp}.log')
    error_handler = RotatingFileHandler(
        error_log_file,
        maxBytes=LOG_FILE_MAX_BYTES,
        backupCount=LOG_FILE_BACKUP_COUNT,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)
    logger.addHandler(error_handler)
    
    # loggingæ¨¡å—æœ¬èº«æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œé€‚ç”¨äºasyncioå’Œå¤šçº¿ç¨‹ç¯å¢ƒ
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    logger.info(f"è¯¦ç»†æ—¥å¿—: {detail_log_file}")
    logger.info(f"é”™è¯¯æ—¥å¿—: {error_log_file}")
    
    return logger


# åˆ›å»ºå…¨å±€logger
logger = setup_logger()

# å¯¼å…¥åŸå§‹çš„ HierarchicalEntityExtractor
from test_submit import HierarchicalEntityExtractor


class ConcurrentExtractor:
    """å¹¶å‘å®ä½“æŠ½å–å™¨"""
    
    def __init__(self, max_concurrent: int = MAX_CONCURRENT_TASKS):
        """
        åˆå§‹åŒ–å¹¶å‘æŠ½å–å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
        """
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.extractor = HierarchicalEntityExtractor()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'retried': 0,
            'start_time': None,
            'end_time': None,
            'errors': []
        }


class ConcurrentExtractorWithJSONL(ConcurrentExtractor):
    """æ”¯æŒ JSONL å®æ—¶ä¿å­˜çš„å¹¶å‘æŠ½å–å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    
    def __init__(self, max_concurrent: int = MAX_CONCURRENT_TASKS, jsonl_file: str = None):
        """
        åˆå§‹åŒ–æ”¯æŒ JSONL çš„å¹¶å‘æŠ½å–å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
            jsonl_file: JSONL è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        super().__init__(max_concurrent)
        self.jsonl_file = jsonl_file
        # asyncio.Lock ç”¨äºä¿æŠ¤æ–‡ä»¶å†™å…¥æ“ä½œï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
        self.file_lock = asyncio.Lock()
    
    async def process_single_sample(
        self, 
        sample: Dict[str, Any], 
        index: int, 
        retry_count: int = 0
    ) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬å¹¶ç«‹å³å†™å…¥ JSONL æ–‡ä»¶
        
        Args:
            sample: è¾“å…¥æ ·æœ¬
            index: æ ·æœ¬ç´¢å¼•
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            å¤„ç†ç»“æœæˆ– None
        """
        async with self.semaphore:
            try:
                # æ·»åŠ è¯·æ±‚å»¶è¿Ÿä»¥é¿å…è¶…è¿‡APIé™åˆ¶
                if index > 0:
                    await asyncio.sleep(REQUEST_DELAY)
                
                sentence = sample['sentence']
                coarse_types = sample['coarse_types']
                
                # è®°å½•è¿›åº¦
                logger.info(f"[{index + 1}/{self.stats['total']}] å¼€å§‹å¤„ç†: {sentence[:50]}...")
                
                # æ‰§è¡Œå®ä½“æŠ½å–
                logger.debug(f"[{index + 1}] è°ƒç”¨å®ä½“æŠ½å–æ–¹æ³•...")
                entities = await self.extractor.extract_entities_for_sentence(
                    sentence, 
                    coarse_types
                )
                logger.info(f"[{index + 1}] æŠ½å–å®Œæˆï¼Œè·å¾— {len(entities)} ä¸ªå®ä½“")
                
                # æ¸…ç†å®ä½“å­—æ®µ
                cleaned_entities = []
                for entity in entities:
                    cleaned_entity = {
                        'name': entity['name'],
                        'coarse_type': entity['coarse_type'],
                        'fine_type': entity['fine_type']
                    }
                    # if 'description' in entity and entity['description']:
                    #     cleaned_entity['description'] = entity['description']
                    cleaned_entities.append(cleaned_entity)
                
                # Preserve original id or generate new one
                sample_id = sample.get('id', str(uuid.uuid4()))
                
                result = {
                    "id": sample_id,
                    "sentence": sentence,
                    "entities": cleaned_entities
                }
                
                # ç«‹å³å†™å…¥ JSONL æ–‡ä»¶ï¼ˆä½¿ç”¨é”ä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
                if self.jsonl_file:
                    logger.debug(f"[{index + 1}] å‡†å¤‡å†™å…¥JSONLæ–‡ä»¶: {self.jsonl_file}")
                    try:
                        async with self.file_lock:
                            with open(self.jsonl_file, 'a', encoding='utf-8') as f:
                                json_line = json.dumps(result, ensure_ascii=False) + '\n'
                                f.write(json_line)
                                f.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº
                        logger.info(f"[{index + 1}] âœ… æˆåŠŸå†™å…¥JSONLæ–‡ä»¶")
                    except Exception as write_error:
                        logger.error(f"[{index + 1}] âŒ å†™å…¥JSONLå¤±è´¥: {str(write_error)}")
                        raise
                else:
                    logger.warning(f"[{index + 1}] âš ï¸  æœªæŒ‡å®šJSONLæ–‡ä»¶ï¼Œè·³è¿‡å†™å…¥")
                
                self.stats['success'] += 1
                logger.info(f"[{index + 1}] âœ… æ ·æœ¬å¤„ç†æˆåŠŸ")
                return result
                
            except Exception as e:
                error_msg = f"æ ·æœ¬ {index + 1} å¤„ç†å¤±è´¥: {str(e)}"
                logger.error(error_msg, exc_info=True)  # æ·»åŠ å®Œæ•´çš„å¼‚å¸¸å †æ ˆ
                
                # è®°å½•é”™è¯¯
                self.stats['errors'].append({
                    'index': index,
                    'sentence': sample.get('sentence', '')[:100],
                    'error': str(e),
                    'retry_count': retry_count
                })
                
                # é‡è¯•é€»è¾‘
                if retry_count < RETRY_TIMES:
                    logger.warning(f"é‡è¯•æ ·æœ¬ {index + 1} (å°è¯• {retry_count + 1}/{RETRY_TIMES})...")
                    self.stats['retried'] += 1
                    await asyncio.sleep(RETRY_DELAY)
                    return await self.process_single_sample(sample, index, retry_count + 1)
                else:
                    self.stats['failed'] += 1
                    logger.error(f"æ ·æœ¬ {index + 1} è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¤„ç†")
                    return None
    
    async def process_dataset_concurrent(
        self, 
        data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        å¹¶å‘å¤„ç†æ•´ä¸ªæ•°æ®é›†
        
        Args:
            data: è¾“å…¥æ•°æ®åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        self.stats['total'] = len(data)
        self.stats['start_time'] = time.time()
        
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†")
        logger.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(data)}")
        logger.info(f"âš¡ æœ€å¤§å¹¶å‘æ•°: {self.max_concurrent}")
        logger.info("=" * 80)
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            self.process_single_sample(sample, i)
            for i, sample in enumerate(data)
        ]
        logger.info(f"ğŸ“‹ å·²åˆ›å»º {len(tasks)} ä¸ªä»»åŠ¡")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info("â³ å¼€å§‹å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡...")
        results = await asyncio.gather(*tasks, return_exceptions=False)
        logger.info(f"âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
        valid_results = [r for r in results if r is not None]
        logger.info(f"âœ… æœ‰æ•ˆç»“æœ: {len(valid_results)}/{len(results)}")
        
        self.stats['end_time'] = time.time()
        
        return valid_results
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        elapsed_time = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("=" * 80)
        logger.info("ğŸ“ˆ å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 80)
        logger.info(f"âœ… æˆåŠŸ: {self.stats['success']}/{self.stats['total']} "
                   f"({self.stats['success']/self.stats['total']*100:.1f}%)")
        logger.info(f"âŒ å¤±è´¥: {self.stats['failed']}/{self.stats['total']} "
                   f"({self.stats['failed']/self.stats['total']*100:.1f}%)")
        logger.info(f"ğŸ”„ é‡è¯•æ¬¡æ•°: {self.stats['retried']}")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {self.stats['total']/elapsed_time:.2f} æ ·æœ¬/ç§’")
        logger.info("=" * 80)
        
        # ç»Ÿè®¡å®ä½“æ•°é‡
        if self.stats.get('results'):
            total_entities = sum(len(r['entities']) for r in self.stats['results'])
            avg_entities = total_entities / len(self.stats['results']) if self.stats['results'] else 0
            logger.info("ğŸ“Š å®ä½“ç»Ÿè®¡:")
            logger.info(f"   æ€»å®ä½“æ•°: {total_entities}")
            logger.info(f"   å¹³å‡æ¯å¥: {avg_entities:.2f} ä¸ªå®ä½“")
            logger.info("=" * 80)
        
        # æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
        if self.stats['errors']:
            logger.warning("âŒ é”™è¯¯è¯¦æƒ…:")
            for error in self.stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.warning(f"   æ ·æœ¬ {error['index'] + 1}: {error['error']}")
            if len(self.stats['errors']) > 10:
                logger.warning(f"   ... è¿˜æœ‰ {len(self.stats['errors']) - 10} ä¸ªé”™è¯¯")
            logger.info("=" * 80)


def jsonl_to_json(jsonl_file: str, json_file: str, indent: int = 4):
    """
    Convert JSON Lines file to standard JSON format.
    
    Args:
        jsonl_file: Path to input JSONL file
        json_file: Path to output JSON file
        indent: Indentation level for pretty printing
    """
    logger.info(f"ğŸ”„ è½¬æ¢ JSONL åˆ° JSON: {jsonl_file} -> {json_file}")
    
    # Check if file exists
    if not os.path.exists(jsonl_file):
        logger.error(f"âŒ JSONL æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_file}")
        return 0
    
    # Check file size
    file_size = os.path.getsize(jsonl_file)
    logger.info(f"ğŸ“Š JSONL æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
    
    results = []
    line_count = 0
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line_count += 1
            line = line.strip()
            if line:  # Skip empty lines
                try:
                    results.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                    logger.error(f"   å†…å®¹: {line[:100]}...")
    
    logger.info(f"ğŸ“Š JSONL æ–‡ä»¶å…± {line_count} è¡Œï¼Œæœ‰æ•ˆæ•°æ® {len(results)} æ¡")
    
    # Save as standard JSON
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=indent)
    
    logger.info(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(results)} æ¡è®°å½•")
    return len(results)


async def process_dataset_file(
    input_file: str,
    output_file: str,
    max_samples: Optional[int] = None,
    max_concurrent: int = MAX_CONCURRENT_TASKS
):
    """
    å¤„ç†æ•°æ®é›†æ–‡ä»¶ï¼ˆä½¿ç”¨ JSON Lines æ ¼å¼é€æ¡ä¿å­˜ï¼‰
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæœ€ç»ˆ JSON æ ¼å¼ï¼‰
        max_samples: æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨ï¼‰
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
    """
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {input_file}")
    
    # åŠ è½½æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°
    if max_samples:
        data = data[:max_samples]
        logger.info(f"ğŸ“Š é™åˆ¶å¤„ç†å‰ {max_samples} ä¸ªæ ·æœ¬")
    
    # JSONL temporary file
    jsonl_file = output_file.replace('.json', '.jsonl')
    logger.info(f"ğŸ“ JSONL ä¸´æ—¶æ–‡ä»¶: {jsonl_file}")
    
    # Clear/create JSONL file (overwrite mode at start)
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(jsonl_file, 'w', encoding='utf-8') as f:
        pass  # Create empty file
    logger.info(f"âœ… JSONL æ–‡ä»¶å·²åˆ›å»ºå¹¶æ¸…ç©º")
    
    # åˆ›å»ºå¹¶å‘æŠ½å–å™¨ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒ JSONLï¼‰
    extractor = ConcurrentExtractorWithJSONL(max_concurrent=max_concurrent, jsonl_file=jsonl_file)
    logger.info(f"âœ… æŠ½å–å™¨å·²åˆ›å»ºï¼ŒJSONLæ–‡ä»¶è·¯å¾„: {extractor.jsonl_file}")
    
    # å¹¶å‘å¤„ç†ï¼ˆç»“æœç›´æ¥å†™å…¥ JSONLï¼‰
    results = await extractor.process_dataset_concurrent(data)
    
    logger.info(f"âœ… JSONL æ–‡ä»¶å·²ä¿å­˜: {jsonl_file}")
    logger.info(f"ğŸ“Š å…±å¤„ç† {len(results)} æ¡è®°å½•")
    
    # Convert JSONL to standard JSON
    logger.info(f"\nğŸ”„ è½¬æ¢ä¸ºæ ‡å‡† JSON æ ¼å¼...")
    total_records = jsonl_to_json(jsonl_file, output_file, indent=4)
    
    logger.info(f"âœ… æœ€ç»ˆç»“æœä¿å­˜åˆ°: {output_file}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    extractor.stats['results'] = results
    extractor.print_statistics()
    
    # ä¿å­˜é”™è¯¯æ—¥å¿—
    if extractor.stats['errors']:
        error_log_file = output_file.replace('.json', '_errors.json')
        with open(error_log_file, 'w', encoding='utf-8') as f:
            json.dump(extractor.stats['errors'], f, ensure_ascii=False, indent=2)
        logger.warning(f"ğŸ“‹ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {error_log_file}")
    
    logger.info("âœ¨ å¤„ç†å®Œæˆ!")


async def main():
    """ä¸»å‡½æ•°"""
    # å¤„ç†å‰3ä¸ªæ ·æœ¬æµ‹è¯•
    await process_dataset_file(
        input_file=INPUT_FILE,
        output_file=OUTPUT_FILE,
        max_samples=10,  # æµ‹è¯•ï¼šåªå¤„ç†å‰3ä¸ªæ ·æœ¬
        max_concurrent=MAX_CONCURRENT_TASKS
    )


async def process_full_dataset():
    """å¤„ç†å®Œæ•´æ•°æ®é›†"""
    logger.warning("âš ï¸  è­¦å‘Š: å³å°†å¤„ç†å®Œæ•´æ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´...")
    response = input("æ˜¯å¦ç»§ç»­? (y/N): ")
    
    if response.lower() == 'y':
        logger.info("å¼€å§‹å¤„ç†å®Œæ•´æ•°æ®é›†...")
        await process_dataset_file(
            input_file=INPUT_FILE,
            output_file=OUTPUT_FILE,
            max_samples=None,  # å¤„ç†å…¨éƒ¨
            max_concurrent=MAX_CONCURRENT_TASKS
        )
    else:
        logger.info("âŒ ç”¨æˆ·å–æ¶ˆå¤„ç†")


if __name__ == "__main__":
    # å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©è¿è¡Œæ¨¡å¼
    
    # æ¨¡å¼1: æµ‹è¯•æ¨¡å¼ï¼ˆå¤„ç†å‰Nä¸ªæ ·æœ¬ï¼‰
    asyncio.run(main())
    
    # æ¨¡å¼2: å®Œæ•´å¤„ç†ï¼ˆå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼‰
    # asyncio.run(process_full_dataset())

